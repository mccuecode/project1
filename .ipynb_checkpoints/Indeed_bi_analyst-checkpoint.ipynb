{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import nltk\n",
    "\n",
    "def get_job_links_page(page):\n",
    "    base_url = \"https://www.indeed.com/jobs?\"\n",
    "    params = {'q': 'BI analyst', # data analist, machine learning engineer, BI analyst\n",
    "             'l': 'Denver, CO'}\n",
    "\n",
    "    # start = \"https://www.indeed.com/jobs?q=Data+Scientist&l=Denver%2C+CO\"\n",
    "    # use a fake header\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.87 Safari/537.36'}\n",
    "    params['start'] = 10 * (page-1)\n",
    "\n",
    "    page = requests.get(base_url, params=params, headers=headers)\n",
    "    # test = requests.get(start, headers=headers)\n",
    "    \n",
    "    soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "    links = soup.find_all(\"a\")\n",
    "    \n",
    "    # build a list of links\n",
    "    some_links = []\n",
    "\n",
    "    for l in links:\n",
    "        try:\n",
    "            hyperlink = l.attrs.get('href')\n",
    "            if \"/rc/clk?\" in hyperlink:\n",
    "                some_links.append(l.attrs.get('href'))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    job_links = [\"https://www.indeed.com{}\".format(x)\n",
    "             for x in some_links\n",
    "             ]\n",
    "    \n",
    "    return job_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = [get_job_links_page(x) for x in range(1, 40)]\n",
    "# bi_analyst_title_links = [get_job_links_page(x, 'BI Analyst') for x in range(1, 20)]\n",
    "\n",
    "# bi_analyst = [get_job_links_page(x, 'BI_Analyst') for x in range(1, 20)]\n",
    "# tableau = [get_job_links_page(x, 'tableau') for x in range(1, 20)]\n",
    "\n",
    "# Add all those lists together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.indeed.com/rc/clk?jk=386eb67cf7620ae5&fccid=093428069580c202&vjs=3'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "merged = list(itertools.chain(*links))\n",
    "len(merged)\n",
    "merged[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'386eb67cf7620ae5'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged[0].split(\"?\")[1:][0].split(\"jk=\")[-1].split(\"&\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "from urllib.parse import parse_qs\n",
    "\n",
    "def get_filename_from_url(some_url):\n",
    "    parsed = parse_qs(some_url)\n",
    "    fccid =  parsed.get('fccid')[0]\n",
    "    other_id = parsed.get('https://www.indeed.com/rc/clk?jk')[0]\n",
    "    return fccid+other_id+ \".html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'093428069580c202386eb67cf7620ae5.html'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_filename_from_url(merged[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set([get_filename_from_url(x) for x in merged])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_job_page(link):\n",
    "    save_name = get_filename_from_url(link)\n",
    "    \n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.87 Safari/537.36'}\n",
    "    test = requests.get(link, headers=headers)\n",
    "    soup = BeautifulSoup(test.text, \"html.parser\")\n",
    "    \n",
    "    with open('business_analyst_{}'.format(save_name), 'w') as f:\n",
    "        f.write(str(soup))\n",
    "    #spans = soup.find_all('span')\n",
    "    #spans_w_divs = [span.find_all('div') for span in spans if len(span.find_all('div')) > 0]\n",
    "    #span = soup.find(\"span\", id=\"job_summary\")\n",
    "    #return str(span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 339/339 [08:14<00:00,  1.46s/it]\n"
     ]
    }
   ],
   "source": [
    "# Download the pages for all the links\n",
    "\n",
    "for link in tqdm(merged):\n",
    "    try:\n",
    "        download_job_page(link)\n",
    "    except Exception as e:\n",
    "        print(str(e), link)\n",
    "    finally:\n",
    "        time.sleep(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "228"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the ref to the files\n",
    "import glob\n",
    "\n",
    "html = glob.glob('*.html')\n",
    "len(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "228"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html_divs = []\n",
    "\n",
    "for html_file in html:\n",
    "    with open(html_file, 'r') as f:\n",
    "        _data = BeautifulSoup(f.read(), \"html.parser\")\n",
    "        try:\n",
    "            has_span_job_summary = _data.find(\"span\", id=\"job_summary\").get_text()\n",
    "            html_divs.append(has_span_job_summary)\n",
    "        except:\n",
    "            print('hh')\n",
    "        \n",
    "len(html_divs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>skill_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>geodatabase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e2e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>analysts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>applied mathematics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            skill_name\n",
       "0          geodatabase\n",
       "1                  e2e\n",
       "2             analysts\n",
       "3                 hive\n",
       "4  applied mathematics"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "skills = pd.read_csv('skill_phrases-JBM.csv', encoding=\"ISO-8859-1\")\n",
    "skills.columns = ['skill_name']\n",
    "skills['skill_name'] = skills.skill_name.map(lambda x: \n",
    "                                           x.lower().strip())\n",
    "skills.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#pd.DataFrame(html_divs).to_csv('parsed_job_desc_jj.csv', header=False, index=False)\n",
    "\n",
    "current_skill = 'python'\n",
    "list_of_skills = skills.skill_name.tolist() # All skills we know of\n",
    "list_of_skills = [x.replace(\"\\xa0\", \" \").replace(\"\\x92s\", \" \") for x in list_of_skills]\n",
    "# https://stackoverflow.com/questions/10993612/python-removing-xa0-from-string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Analyst\\nCommunity Reach Center (CRC) is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who We Are:\\n\\n\\nBall Corporation, since its f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Position: BI/Reporting Developer\\n\\nJob Descri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Client Relations Reporting Specialist\\n\\nCompu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Job Description\\nWe are looking to hire a full...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                desc\n",
       "0  Data Analyst\\nCommunity Reach Center (CRC) is ...\n",
       "1  Who We Are:\\n\\n\\nBall Corporation, since its f...\n",
       "2  Position: BI/Reporting Developer\\n\\nJob Descri...\n",
       "3  Client Relations Reporting Specialist\\n\\nCompu...\n",
       "4  Job Description\\nWe are looking to hire a full..."
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs = pd.DataFrame(html_divs)\n",
    "jobs.columns = ['desc']\n",
    "jobs.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  Searched in:\n    - '/Users/CSR/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/Applications/anaconda3/nltk_data'\n    - '/Applications/anaconda3/share/nltk_data'\n    - '/Applications/anaconda3/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-5d0881b062b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mjob_desc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mjob_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mjob_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjob_tokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     job_tokens = [word.lower().strip() for word \n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserver_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \"\"\"\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     return [token for sent in sentences\n\u001b[1;32m    130\u001b[0m             for token in _treebank_word_tokenizer.tokenize(sent)]\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \"\"\"\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 836\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'raw'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nltk'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    955\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'file'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  Searched in:\n    - '/Users/CSR/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/Applications/anaconda3/nltk_data'\n    - '/Applications/anaconda3/share/nltk_data'\n    - '/Applications/anaconda3/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from collections import defaultdict\n",
    "\n",
    "found = 0\n",
    "\n",
    "for index, row in jobs.iterrows():\n",
    "    job_desc = row.desc\n",
    "    job_tokens = nltk.word_tokenize(job_desc)\n",
    "    job_tokens = [word for word in job_tokens if len(word) >= 1]\n",
    "    job_tokens = [word.lower().strip() for word \n",
    "                  in job_tokens if not word in string.punctuation ]\n",
    "    # repeat and say if word not in stop_words_ist\n",
    "    has_py = \"sql\" in job_tokens\n",
    "    if has_py:\n",
    "        found +=1\n",
    "        #am I always matching python right? seems low occurrence to me...\n",
    "        # find all other skills from list of tokens \n",
    "        # var name list_of_skills\n",
    "        matches = defaultdict(int)\n",
    "        \n",
    "        for skill in list_of_skills:\n",
    "            if skill in job_tokens:\n",
    "                matches[skill] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(found)\n",
    "matches #10 - 100x more data\n",
    "sorted(matches.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "#nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# END JJ CODE\n",
    "\n",
    "\n",
    "import csv\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "def initialize_highlighting(filename):\n",
    "# Read a list of skill phrases from a file\n",
    "    skill_phrases = []\n",
    "    with open(filename, 'r', newline='') as csvfile:\n",
    "        csv_reader = csv.reader(csvfile)\n",
    "        for row in csv_reader:\n",
    "            skill_phrases.append(str(*row).strip())\n",
    "    skill_phrases = set(skill_phrases)\n",
    "\n",
    "    # Write this clean version back out\n",
    "    with open('skill_phrases_out.csv', 'w', newline='') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        for skill_phrase in skill_phrases:\n",
    "            csv_writer.writerow([skill_phrase])\n",
    "        \n",
    "    #separate each skill phrase into a list of its words\n",
    "    from nltk.tokenize import RegexpTokenizer\n",
    "    tokenizer = RegexpTokenizer(r'[a-zA-Z0-9#+-]+')\n",
    "    skill_phrase_wl = [tokenizer.tokenize(skill_phrase) for skill_phrase in skill_phrases]\n",
    "    \n",
    "    return skill_phrase_wl\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "def tally_skill_mentions_in_job(t, skill_phrase_wl):\n",
    "    \n",
    "    skill_mentions_in_job = defaultdict(int)\n",
    "    # tokenize the text of the description, without spans\n",
    "    tokenizer = RegexpTokenizer(r'[a-zA-Z0-9#+-]+')\n",
    "    tokens = tokenizer.tokenize(t)\n",
    "    # create a dictionary of the words in the job description\n",
    "    word_index = defaultdict(list)\n",
    "    for i, k in enumerate(tokens):\n",
    "        word_index[k].append(i)\n",
    "    \n",
    "    # search the word_index dictionary to find the fist word of each skill_phrase\n",
    "    for skill_phrase in skill_phrase_wl:\n",
    "        if word_index.get(skill_phrase[0]):\n",
    "            for occurence in word_index.get(skill_phrase[0]):\n",
    "                # Check to see if the whole phrase matches\n",
    "                if all((skill_phrase[j] == tokens[j+occurence]) for j in range(len(skill_phrase))):\n",
    "                    skill_mentions_in_job[tuple(skill_phrase)] += 1\n",
    "    return skill_mentions_in_job        \n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "# requires the global variable skill_phrase_wl\n",
    "# maybe this should be a parameter rather than a global\n",
    "def highlight_phrases_from_list(t):\n",
    "\n",
    "    # tokenize the text of the description, with spans\n",
    "    tokenizer = RegexpTokenizer(r'[a-zA-Z0-9#+-]+')\n",
    "    span_generator = tokenizer.span_tokenize(t)\n",
    "    spans = [span for span in span_generator]\n",
    "    tokens = [t[span[0]:span[1]] for span in spans]\n",
    "    \n",
    "#     # create a dictionary of the words, with spans as the values\n",
    "#     # and another dictionary with the same keys, with the word indexes as the values\n",
    "    char_span = defaultdict(list)\n",
    "    word_index = defaultdict(list)\n",
    "    for i, (k, span) in enumerate(zip(tokens, spans)):\n",
    "        char_span[k].append(span)\n",
    "        word_index[k].append(i)\n",
    "    # is this useful?\n",
    "    df = pd.DataFrame({'Character Index Spans': pd.Series(char_span), 'Word Indexes': pd.Series(word_index)})\n",
    "\n",
    "    highlight_spans = []   \n",
    "    for skill_phrase in skill_phrase_wl:\n",
    "        if word_index.get(skill_phrase[0]):\n",
    "            for i, occurence in enumerate(word_index.get(skill_phrase[0])):\n",
    "                if all((skill_phrase[j] == tokens[j+occurence]) for j in range(len(skill_phrase))):\n",
    "                    highlight_span = (spans[occurence][0], spans[occurence + len(skill_phrase) - 1][1])\n",
    "                    highlight_spans.append (highlight_span)\n",
    "\n",
    "# # look up the words in our skill list in the dictionary.  List the findings as spans to be highlighted\n",
    "#     for skill in single_word_skills:\n",
    "#         highlight_spans += char_span[skill]\n",
    "\n",
    "    # Sort the spans to be highlighted\n",
    "    highlight_spans.sort()\n",
    "\n",
    "    # Insert html tags to highlight the keywords\n",
    "    html_start_tag = '<font color=\"red\">'\n",
    "    html_end_tag = '</font>'\n",
    "    highlighted = ''\n",
    "    cursor = 0\n",
    "    for span in highlight_spans:\n",
    "        if (span[0] > cursor): # go forwards only, not backwards \n",
    "            if (cursor>0):\n",
    "                highlighted += html_end_tag\n",
    "            highlighted += t[cursor:span[0]] +                             html_start_tag +                             t[span[0]:span[1]]\n",
    "        elif (span[1] > cursor):\n",
    "            highlighted += t[cursor:span[1]]\n",
    "        cursor = span[1]\n",
    "    highlighted += html_end_tag + t[cursor:]\n",
    "    display(HTML(highlighted))    \n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "job_links = []\n",
    "for page in range(1,25):\n",
    "    job_links+=(get_job_links_page(page))\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "tallied_skill_mentions = []\n",
    "skill_phrase_wl = initialize_highlighting('skill_phrases_purged-JBM.csv')\n",
    "skill_dict = {tuple(skill_phrase): 0 for skill_phrase in skill_phrase_wl}\n",
    "for link in tqdm(job_links):\n",
    "    # highlight_phrases_from_list(get_job_summary(link))\n",
    "    tallied_skill_mentions.append(tally_skill_mentions_in_job(get_job_summary(link), skill_dict))\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "df = pd.DataFrame(tallied_skill_mentions, index = job_links).fillna(0).astype(int)\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "skills_mentioned = df.columns.values\n",
    "skill_phrases_mentioned = [' '.join(c) for c in df.columns.values]\n",
    "df.columns = skill_phrases_mentioned\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "df\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "not_mentioned = skill_dict\n",
    "for skill in skills_mentioned:\n",
    "    del not_mentioned[skill]\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "phrases_not_mentioned = [' '.join(s) for s in not_mentioned]\n",
    "phrases_not_mentioned\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "len (phrases_not_mentioned)\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "df.sum(axis = 1).sort_values()\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "skill_phrases = []\n",
    "with open('skill_phrases_purged-JBM.csv', 'r', newline='') as csvfile:\n",
    "    csv_reader = csv.reader(csvfile)\n",
    "    for row in csv_reader:\n",
    "        skill_phrases.append(str(row[]).strip())\n",
    "\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "skill_phrases=list(set(skill_phrases))\n",
    "\n",
    "\n",
    "# In[33]:\n",
    "\n",
    "import random\n",
    "from itertools import compress\n",
    "skill_percentage = random.random()\n",
    "n_skill_phrases = len(skill_phrases)\n",
    "n_my_skills = skill_percentage * n_skill_phrases\n",
    "my_skills_bool = [(x < n_my_skills) for x in range(n_skill_phrases)]\n",
    "random.shuffle(my_skills_bool)\n",
    "my_skill_list = list(compress(skill_phrases,my_skills_bool))\n",
    "my_skill_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
