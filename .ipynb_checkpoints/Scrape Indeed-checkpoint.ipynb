{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_job_links_page(page):\n",
    "    base_url = \"https://www.indeed.com/jobs?\"\n",
    "    params = {'q': 'data scientist',\n",
    "             'l': 'Denver, CO'}\n",
    "\n",
    "    # start = \"https://www.indeed.com/jobs?q=Data+Scientist&l=Denver%2C+CO\"\n",
    "    # use a fake header\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.87 Safari/537.36'}\n",
    "    params['start'] = 10 * (page-1)\n",
    "\n",
    "    page = requests.get(base_url, params=params, headers=headers)\n",
    "    # test = requests.get(start, headers=headers)\n",
    "    \n",
    "    soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "    links = soup.find_all(\"a\")\n",
    "    \n",
    "    # build a list of links\n",
    "    some_links = []\n",
    "\n",
    "    for l in links:\n",
    "        try:\n",
    "            hyperlink = l.attrs.get('href')\n",
    "            if \"/rc/clk?\" in hyperlink:\n",
    "                some_links.append(l.attrs.get('href'))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    job_links = [\"https://www.indeed.com{}\".format(x)\n",
    "             for x in some_links\n",
    "             ]\n",
    "    \n",
    "    return job_links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_summary(link):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.87 Safari/537.36'}\n",
    "    test = requests.get(link, headers=headers)\n",
    "    soup = BeautifulSoup(test.text, \"html.parser\")\n",
    "    spans = soup.find_all('span')\n",
    "    spans_w_divs = [span.find_all('div') for span in spans if len(span.find_all('div')) > 0]\n",
    "    span = soup.find(\"span\", id=\"job_summary\")\n",
    "    return str(span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "def initialize_highlighting(filename):\n",
    "# Read a list of skill phrases from a file\n",
    "    skill_phrases = []\n",
    "    with open(filename, 'r', newline='') as csvfile:\n",
    "        csv_reader = csv.reader(csvfile)\n",
    "        for row in csv_reader:\n",
    "            skill_phrases.append(str(*row).strip())\n",
    "    skill_phrases = set(skill_phrases)\n",
    "\n",
    "    # Write this clean version back out\n",
    "    with open('skill_phrases_out.csv', 'w', newline='') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        for skill_phrase in skill_phrases:\n",
    "            csv_writer.writerow([skill_phrase])\n",
    "        \n",
    "    #separate each skill phrase into a list of its words\n",
    "    from nltk.tokenize import RegexpTokenizer\n",
    "    tokenizer = RegexpTokenizer(r'[a-zA-Z0-9#+-]+')\n",
    "    skill_phrase_wl = [tokenizer.tokenize(skill_phrase) for skill_phrase in skill_phrases]\n",
    "    \n",
    "    return skill_phrase_wl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tally_skill_mentions_in_job(t, skill_phrase_wl):\n",
    "    \n",
    "    skill_mentions_in_job = defaultdict(int)\n",
    "    # tokenize the text of the description, without spans\n",
    "    tokenizer = RegexpTokenizer(r'[a-zA-Z0-9#+-]+')\n",
    "    tokens = tokenizer.tokenize(t)\n",
    "    # create a dictionary of the words in the job description\n",
    "    word_index = defaultdict(list)\n",
    "    for i, k in enumerate(tokens):\n",
    "        word_index[k].append(i)\n",
    "    \n",
    "    # search the word_index dictionary to find the fist word of each skill_phrase\n",
    "    for skill_phrase in skill_phrase_wl:\n",
    "        if word_index.get(skill_phrase[0]):\n",
    "            for occurence in word_index.get(skill_phrase[0]):\n",
    "                # Check to see if the whole phrase matches\n",
    "                if all((skill_phrase[j] == tokens[j+occurence]) for j in range(len(skill_phrase))):\n",
    "                    skill_mentions_in_job[tuple(skill_phrase)] += 1\n",
    "    return skill_mentions_in_job        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "# requires the global variable skill_phrase_wl\n",
    "# maybe this should be a parameter rather than a global\n",
    "def highlight_phrases_from_list(t):\n",
    "\n",
    "    # tokenize the text of the description, with spans\n",
    "    tokenizer = RegexpTokenizer(r'[a-zA-Z0-9#+-]+')\n",
    "    span_generator = tokenizer.span_tokenize(t)\n",
    "    spans = [span for span in span_generator]\n",
    "    tokens = [t[span[0]:span[1]] for span in spans]\n",
    "    \n",
    "#     # create a dictionary of the words, with spans as the values\n",
    "#     # and another dictionary with the same keys, with the word indexes as the values\n",
    "    char_span = defaultdict(list)\n",
    "    word_index = defaultdict(list)\n",
    "    for i, (k, span) in enumerate(zip(tokens, spans)):\n",
    "        char_span[k].append(span)\n",
    "        word_index[k].append(i)\n",
    "    # is this useful?\n",
    "    df = pd.DataFrame({'Character Index Spans': pd.Series(char_span), 'Word Indexes': pd.Series(word_index)})\n",
    "\n",
    "    highlight_spans = []   \n",
    "    for skill_phrase in skill_phrase_wl:\n",
    "        if word_index.get(skill_phrase[0]):\n",
    "            for i, occurence in enumerate(word_index.get(skill_phrase[0])):\n",
    "                if all((skill_phrase[j] == tokens[j+occurence]) for j in range(len(skill_phrase))):\n",
    "                    highlight_span = (spans[occurence][0], spans[occurence + len(skill_phrase) - 1][1])\n",
    "                    highlight_spans.append (highlight_span)\n",
    "\n",
    "# # look up the words in our skill list in the dictionary.  List the findings as spans to be highlighted\n",
    "#     for skill in single_word_skills:\n",
    "#         highlight_spans += char_span[skill]\n",
    "\n",
    "    # Sort the spans to be highlighted\n",
    "    highlight_spans.sort()\n",
    "\n",
    "    # Insert html tags to highlight the keywords\n",
    "    html_start_tag = '<font color=\"red\">'\n",
    "    html_end_tag = '</font>'\n",
    "    highlighted = ''\n",
    "    cursor = 0\n",
    "    for span in highlight_spans:\n",
    "        if (span[0] > cursor): # go forwards only, not backwards \n",
    "            if (cursor>0):\n",
    "                highlighted += html_end_tag\n",
    "            highlighted += t[cursor:span[0]] + \\\n",
    "                            html_start_tag + \\\n",
    "                            t[span[0]:span[1]]\n",
    "        elif (span[1] > cursor):\n",
    "            highlighted += t[cursor:span[1]]\n",
    "        cursor = span[1]\n",
    "    highlighted += html_end_tag + t[cursor:]\n",
    "    display(HTML(highlighted))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_links = []\n",
    "for page in range(1,25):\n",
    "    job_links+=(get_job_links_page(page))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xa0 in position 1695: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-2d7f94c425bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtallied_skill_mentions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mskill_phrase_wl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_highlighting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'skill_phrases_purged-JBM.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mskill_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskill_phrase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mskill_phrase\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mskill_phrase_wl\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlink\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_links\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-ba38761fe82b>\u001b[0m in \u001b[0;36minitialize_highlighting\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcsvfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mcsv_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsvfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcsv_reader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0mskill_phrases\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mskill_phrases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskill_phrases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0;31m# keep undecoded input until the next call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconsumed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xa0 in position 1695: invalid start byte"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "tallied_skill_mentions = []\n",
    "skill_phrase_wl = initialize_highlighting('skill_phrases_purged-JBM.csv')\n",
    "skill_dict = {tuple(skill_phrase): 0 for skill_phrase in skill_phrase_wl}\n",
    "for link in tqdm(job_links):\n",
    "    # highlight_phrases_from_list(get_job_summary(link))\n",
    "    tallied_skill_mentions.append(tally_skill_mentions_in_job(get_job_summary(link), skill_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(tallied_skill_mentions, index = job_links).fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skills_mentioned = df.columns.values\n",
    "skill_phrases_mentioned = [' '.join(c) for c in df.columns.values]\n",
    "df.columns = skill_phrases_mentioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_mentioned = skill_dict\n",
    "for skill in skills_mentioned:\n",
    "    del not_mentioned[skill]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases_not_mentioned = [' '.join(s) for s in not_mentioned]\n",
    "phrases_not_mentioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len (phrases_not_mentioned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sum(axis = 1).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_phrases = []\n",
    "with open('skill_phrases_purged-JBM.csv', 'r', newline='') as csvfile:\n",
    "    csv_reader = csv.reader(csvfile)\n",
    "    for row in csv_reader:\n",
    "        skill_phrases.append(str(row[]).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_phrases=list(set(skill_phrases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from itertools import compress\n",
    "skill_percentage = random.random()\n",
    "n_skill_phrases = len(skill_phrases)\n",
    "n_my_skills = skill_percentage * n_skill_phrases\n",
    "my_skills_bool = [(x < n_my_skills) for x in range(n_skill_phrases)]\n",
    "random.shuffle(my_skills_bool)\n",
    "my_skill_list = list(compress(skill_phrases,my_skills_bool))\n",
    "my_skill_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
